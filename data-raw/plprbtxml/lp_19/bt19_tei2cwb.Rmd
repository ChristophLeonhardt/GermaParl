---
title: "Bundestag, 18th Electoral Period: From TEI to CWB"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Bundestag 18}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---


```{r starting_time}
starting_time <- Sys.time()
```

## Introduction

This is a template for encoding projects for corpora of plenary protocols (TEI input format). The project is configured in the following section, the following procedure is generic.

The Rmarkdown document can be executed by using `Rscript` from a terminal. The corpus will be prepared when preparing the output html document, which is a reproducible documentation of the annotation and indexing procedure.

```{sh command_line_use, eval = FALSE}
Rscript -e 'rmarkdown::render(input = "bt19_tei2cwb.Rmd", output_file = "bt19_tei_to_cwb.html")'
```

## Configure the encoding project

### The corpus to process

```{r define_corpus}
corpus <- "BT19"
```

### Files to be processed

```{r get_files_to_process}
teidir <- "~/Lab/tmp/btxml"
dir.exists(teidir)
teifiles <- Sys.glob(file.path(teidir, "*.xml"))
length(teifiles)
```

For trial runs, set a number of sample documents that will be drawn from all TEI files. To process all documents, comment out the line.

```{r subset_of_files}
# teisample <- 3
if (exists("teisample")) teifiles <- sample(teifiles, size = teisample)
length(teifiles)
```


### Multicore

Number of cores to use for running annotators.

```{r no_of_cores}
no_cores <- 1L
```


### Stanford CoreNLP configuration

Define properties file used to configure Stanford CoreNLP.

```{r get_properties_file}
language <- "de"
properties_file <- system.file(package = "bignlp",
            "extdata", "properties_files",
            "corenlp-german-fast.properties"
            )
properties_file
```

```{r install_corenlp_if_necessary}
if (bignlp::corenlp_get_jar_dir() == "") bignlp::corenlp_install(lang = language)
```

### CWB input required

```{r define_p_attributes}
p_attrs <- c("word", "pos")
```

We get the registry directory and remove the registry file for the newly encoded corpus, if it already exists. 

```{r registry_directory}
registry <- Sys.getenv("CORPUS_REGISTRY")
if (!file.exists(registry)) stop("environment variable CORPUS_REGISTRY needs to be defined")
registry_file <- file.path(registry, tolower(corpus))
if (file.exists(registry_file)) file.remove(stateparl_registry_file)
```

We create a project-specific data directory within the data directory and make sure it is empty.
  
```{r data_directory}
data_dir <- file.path(dirname(registry), "indexed_corpora")
if (!file.exists(data_dir)) dir.create(data_dir)
bt_data_dir <- file.path(data_dir, tolower(corpus))
if (!file.exists(bt_data_dir)) dir.create(bt_data_dir)
file.remove(list.files(bt_data_dir, full.names = TRUE))
```



## Preparatory steps


### Load required packages

```{r load_libraries}
library(data.table)
library(magrittr)
library(cwbtools)
library(bignlp)
```



```{r java_options}
options(java.parameters = "-Xmx4g") # needs to be set before a JVM is initialized.
options(bignlp.properties_file = properties_file)
```


### Meet installation requirements

```{r install_cwb}
if (!cwb_is_installed()) cwb_install()
```


### The directory and files for the NLP pipe

```{r tempfiles}
tmpdir <- tempdir()
if (.Platform$OS.type == "windows") tmpdir <- normalizePath(tmpdir, winslash = "/")

nlp_dir <- file.path(tmpdir, "nlp_dir")
if (!file.exists(nlp_dir)) dir.create(nlp_dir)

chunkdata_file <- file.path(nlp_dir, paste(tolower(corpus), "tsv", sep = "."))
ndjson_file <- file.path(nlp_dir, paste(tolower(corpus), "ndjson", sep = "."))
tsv_file_tagged <- file.path(nlp_dir, paste(tolower(corpus), "tagged.tsv", sep = "_"))
```

```{r}
if (file.exists(chunkdata_file)) file.remove(chunkdata_file)
if (file.exists(ndjson_file)) file.remove(ndjson_file)
if (file.exists(tsv_file_tagged)) file.remove(tsv_file_tagged)
```


## From XML to CWB

### Initialize CorpusData class

```{r initialize_corpus_data}
CD <- CorpusData$new()

metadata <- c(
  lp = "//legislativePeriod", session = "//titleStmt/sessionNo",
  date = "//publicationStmt/date", url = "//sourceDesc/url",
  src = "//sourceDesc/filetype"
)
```

### Read in data

```{r read_xml_files}
CD$import_xml(filenames = teifiles, meta = metadata, progress = interactive())
CD
```

### Rework metadata table

The TEI files include an element "speaker" that is redundant and can (and should be) dropped.

```{r remove_speaker_element}
to_keep <- which(is.na(CD$metadata[["speaker"]]))
CD$chunktable <- CD$chunktable[to_keep]
CD$metadata <- CD$metadata[to_keep][, speaker := NULL]
```

```{r adjust_colnames}
setnames(CD$metadata, old = c("sp_who", "sp_party", "sp_role"), new = c("who", "party", "role"))
```


### Linguistic annotation

```{r write_chunktable_to_disk}
fwrite(x = CD$chunktable, file = chunkdata_file, sep = "\t")
```


```{r annotate_and_parse}
chunkfiles <-  chunk_table_split(chunkdata_file, output = NULL, n = no_cores, verbose = TRUE)
ndjsonfiles <-  corenlp_annotate(chunkfiles, threads = no_cores, byline = TRUE, progress = interactive())
tsvfiles <- corenlp_parse_ndjson(
  ndjsonfiles, cols_to_keep = c("id", p_attrs), output = tsv_file_tagged,
  threads = no_cores, progress = interactive()
  )
dt_list <- lapply(tsvfiles, fread)
CD$tokenstream <- rbindlist(dt_list)
```



```{r add_corpus_positions}
CD$tokenstream[, "id" := as.character(CD$tokenstream[["id"]])]
CD$metadata[, "id" := as.character(CD$metadata[["id"]])]
CD$add_corpus_positions()
setorderv(CD$metadata, cols = "cpos_left", order = 1L)
```

```{r s_attributes_to_encode}
s_attrs <- c("id", "who", "party", "role", "lp", "session", "date")
```


```{r encode}
CD$encode(
  registry_dir = registry, data_dir = bt_data_dir,
  corpus = toupper(corpus), encoding = "utf8", method = "CWB",
  p_attributes = "word", s_attributes = s_attrs,
  compress = TRUE
)
```


```{r finished}
Sys.time() - starting_time
```
